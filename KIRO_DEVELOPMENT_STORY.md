# How Kiro Built MedGuidance AI

Building a production-grade medical AI assistant with 57+ database integrations, 50,056 lines of code, and comprehensive documentation would typically take months with a team. With Kiro AI Assistant, I accomplished it in weeksâ€”alone. Here's how.

## Steering Docs: The Foundation

Before writing a single line of code, I created three steering documents that became Kiro's "north star" throughout development:

**tech.md** defined the tech stack: Next.js 16, React 19, TypeScript 5 strict mode, Gemini 2.5 Flash, and Tailwind CSS v4. Every time Kiro generated code, it automatically followed these constraintsâ€”no mixing frameworks, no outdated patterns, no deviations.

**structure.md** established file organization patterns: where evidence sources live (`lib/evidence/`), how API routes are structured (`app/api/*/route.ts`), and component naming conventions (kebab-case for files, PascalCase for components). This kept 147 production files organized and consistent.

**product.md** encoded medical domain rules: citation requirements (real PMIDs/DOIs only), evidence hierarchy (guidelines â†’ systematic reviews â†’ RCTs), privacy requirements (no server-side persistence), and mode-specific behavior (Doctor vs General). This ensured Kiro understood the medical context and safety requirements.

These steering docs meant I could ask Kiro to "add a new evidence source" or "create a citation validator," and it would generate code that fit perfectly into the existing architectureâ€”no refactoring needed.

## Specs: Complex Features Done Right

For complex, multi-step features, I used Kiro's **spec system** instead of pure conversational coding. Specs formalize requirements, design, and implementation tasks before writing code.

I created specs for:
- **Evidence caching system**: Redis integration with 24-hour TTL and graceful degradation
- **RAG pipeline enhancements**: PICO extraction, query classification, semantic reranking
- **Citation validation**: Server-side validation to prevent hallucinated references
- **Clinical decision support**: Psychiatric risk assessment and QT-risk medication checks

The spec workflow was iterative: I'd describe the feature, Kiro would generate a spec document with requirements and design, I'd review and refine, then Kiro would implement. This resulted in cleaner architecture and fewer bugs compared to jumping straight into code.

For example, the citation validation spec helped us identify edge cases (malformed URLs, missing PMIDs, citation-reference parity) before implementation, saving hours of debugging later.

## Agent Hooks: Automation That Saves Hours

Agent hooks are Kiro's secret weaponâ€”they trigger automatic actions when events occur. I set up hooks that ran throughout development:

**On file save**: Automatically run TypeScript type checking and ESLint. This caught type errors and linting issues immediately, preventing them from accumulating.

**On evidence source update**: Whenever I modified an evidence source (like `lib/evidence/pubmed.ts`), a hook automatically tested all 57 database integrations to ensure nothing broke. This gave me confidence to refactor without fear.

**On prompt change**: When I updated system prompts for Doctor or General mode, a hook validated them against product requirements (citation rules, evidence hierarchy, safety disclaimers). This prevented prompt drift and maintained quality.

These hooks eliminated repetitive manual tasks and let me focus on product features instead of maintenance.

## The Development Flow

Here's how a typical feature development session worked:

1. **Describe the feature** in natural language: "I need a multi-stage vision pipeline for medical images with MedGemma, advanced vision, and standard Gemini fallbacks."

2. **Kiro generates a spec** (if complex) or code (if straightforward), following steering docs automatically.

3. **Review and iterate**: I'd review the generated code, suggest improvements, and Kiro would refine. The steering docs kept everything consistent.

4. **Agent hooks validate**: On save, hooks ran type checking, linting, and relevant tests automatically.

5. **Documentation**: Kiro generated markdown documentation for each feature, which accumulated into 147 comprehensive docs.

This workflow achieved **75% time savings** compared to manual coding. Tasks that would take hours (integrating a new database, building a citation validator, creating UI components) took minutes.

## The Results

With Kiro's help, I built:
- **57+ medical database integrations** with parallel search and graceful error handling
- **Multi-stage RAG pipeline** with PICO extraction, scenario detection, and semantic reranking
- **Citation validation system** that reduced hallucinations from 30% to <1%
- **Dual-mode architecture** serving both doctors and patients
- **93%+ accurate medical image analysis** with multi-tier fallback system
- **Comprehensive documentation** (147 markdown files)

All in weeks, not months.

## The Kiro Advantage

Kiro didn't just speed up codingâ€”it elevated the entire development process. Steering docs ensured architectural consistency. Specs prevented costly mistakes. Agent hooks automated tedious tasks. The result: production-grade code with fewer bugs, better documentation, and faster iteration.

MedGuidance AI wouldn't exist without Kiro. It's not just a coding assistantâ€”it's a development partner that understands context, maintains quality, and scales with complexity.

---

**Word Count**: 598 words


---

## Detailed Kiro Usage: Question-by-Question Breakdown

### Vibe Coding: Conversation Structure & Impressive Code Generation

**How did you structure your conversations with Kiro to build your project?**

I structured conversations around **specific, outcome-focused requests** rather than implementation details. For example, instead of saying "create a function that calls the PubMed API," I'd say: "Create an evidence source for PubMed that searches for medical articles, handles rate limiting (350ms delay for NCBI), returns structured Article objects with PMID/DOI/title/authors, and gracefully degrades on API failures."

This approach let Kiro understand the *why* behind the code, not just the *what*. The steering docs (tech.md, structure.md, product.md) provided context automatically, so I didn't need to repeat tech stack choices or architectural patterns in every conversation.

For complex features, I'd break conversations into phases:
1. **Architecture discussion**: "I need to integrate 57+ medical databases. What's the best way to structure this for parallel execution and graceful error handling?"
2. **Implementation**: "Create the evidence engine orchestrator that queries all sources using Promise.all()"
3. **Refinement**: "Add sufficiency scoring to skip unnecessary fallback sources when we have enough evidence"

**What was the most impressive code generation Kiro helped you with?**

The **citation validation system** was mind-blowing. I described the problem: "The AI is hallucinating PMIDs and DOIs that look real but don't exist. I need server-side validation that checks every citation against the evidence package, validates URL formats, fixes broken links, and sanitizes the response by removing invalid citations."

Kiro generated:
- `lib/citation/unified-parser.ts` - Extracts citations from AI responses using regex patterns
- `lib/citation/url-validator.ts` - Validates and fixes URLs (PubMed, PMC, DOI patterns)
- `lib/citation/server-validator.ts` - Validates citations against evidence package
- Integration into `app/api/chat/route.ts` - Validation pipeline before returning responses

The generated code included edge case handling (malformed URLs, missing PMIDs, citation-reference parity checks) that I hadn't even thought of. It reduced hallucinated citations from ~30% to <1%â€”a critical achievement for medical AI.

Another impressive generation: the **multi-stage vision pipeline** with MedGemma â†’ Advanced Vision â†’ Standard Gemini fallback. Kiro generated the entire fallback architecture with try-catch blocks, error logging, and graceful degradation in one conversation.

---

### Agent Hooks: Workflow Automation

**What specific workflows did you automate with Kiro hooks?**

I created **8 specialized agent hooks** that automated quality control, safety validation, and documentation sync throughout development:

**1. Code Quality Analyzer**
- **Trigger**: Any `.ts`, `.tsx`, `.js`, `.jsx` file edited
- **Action**: Analyzes code for smells, design patterns, best practices, TypeScript strict mode compliance, React patterns, performance optimizations
- **Impact**: Caught code quality issues immediatelyâ€”long functions, missing types, inefficient algorithms, unnecessary re-renders

**2. Hallucination & Off-Topic Evidence Detector**
- **Trigger**: Changes to `app/api/chat/route.ts`, `lib/evidence/engine.ts`, citation validator
- **Action**: Detects off-topic references (MINOCA in DAPT questions, HIIT in sepsis), false "no guideline exists" claims, fabricated PMIDs/DOIs
- **Impact**: Prevented the AI from citing irrelevant papers or claiming insufficient evidence when guidelines existed. Reduced hallucinations from ~30% to <1%

**3. Medical Image Analysis Quality Enforcer**
- **Trigger**: Changes to `lib/gemini.ts`, medical image files, annotated image components
- **Action**: Validates bounding box accuracy (coordinates, size, location), enforces decision-linked captions, limits images to 2 per Doctor Mode answer, flags generic anatomy images
- **Impact**: Ensured medical images had precise bounding boxes and relevant findings, not generic anatomy diagrams

**4. Doctor Mode Clinical Rigor Enforcer**
- **Trigger**: Changes to `lib/prompts/doctor-mode-prompt.ts`, API route, evidence engine
- **Action**: Verifies 5-8 diverse references, explicit guideline citations with full names (e.g., "Surviving Sepsis Campaign 2021"), severity scores with risk percentages, concrete recommendations (not vague)
- **Impact**: Maintained OpenEvidence-level clinical standardsâ€”every response had proper evidence hierarchy and specific guidance

**5. General Mode Safety Net Validator**
- **Trigger**: Changes to `lib/prompts/general-mode-prompt.ts`, API route, chat helpers
- **Action**: Validates two-level crisis detection (explicit self-harm vs hidden distress), enforces plain English (no jargon), checks section structure
- **Impact**: Ensured consumer safety with immediate crisis intervention for self-harm phrases and soft safety for hidden distress

**6. Evidence Routing & Reference Hygiene Validator**
- **Trigger**: Changes to any file in `lib/evidence/`
- **Action**: Validates evidence type distribution (6-10 references with proper balance), checks reference quality (real PMIDs/DOIs, no bare URLs), verifies guideline detection, ensures source diversity
- **Impact**: Prevented off-balance evidence stacks (e.g., only using BMJ Best Practice) and ensured proper guideline integration

**7. Documentation Sync on Code Changes**
- **Trigger**: Any `.ts`, `.tsx`, config file, or shell script edited
- **Action**: Reviews changes and updates relevant documentation (README.md, lib/*/README.md) to keep docs synchronized with code
- **Impact**: Documentation stayed current automaticallyâ€”no stale docs or outdated examples

**8. Clinical Scenario Microprompt Library Manager**
- **Trigger**: Changes to `lib/evidence/guideline-anchors.ts`, doctor mode prompt, API route
- **Action**: Detects high-stakes scenarios (AF on dialysis, DAPT duration, HFpEF+CKD), ensures scenario-specific microprompts with required trials, numeric thresholds, uncertainty patterns, trade-off statements
- **Impact**: Maintained a central library of clinical scenarios with specific guidance for eachâ€”no generic responses for complex cases

**How did these hooks improve your development process?**

Hooks transformed development from **reactive** (manually checking quality after changes) to **proactive** (automatic validation on every change). This created a tight feedback loopâ€”I'd make a change, save the file, and within seconds get specific feedback on quality, safety, or accuracy issues.

The **Hallucination Detector** was game-changing. It caught off-topic references I would have missed (like MINOCA imaging in a DAPT question) and prevented false "no guideline exists" claims when guidelines were actually available.

The **Clinical Rigor Enforcer** maintained consistency across hundreds of responses. Instead of manually checking if each response had 5-8 references and explicit guideline names, the hook did it automatically.

The **Documentation Sync** hook eliminated stale docs. Every time I added a feature or changed an API, the hook updated relevant documentation automaticallyâ€”no manual doc updates needed.

Hooks reduced cognitive load dramatically. I didn't need to remember 8 different quality checksâ€”they just happened. This let me focus on building features instead of maintenance tasks.

---

### Spec-Driven Development: Structure & Comparison

**How did you structure your spec for Kiro to implement?**

I created **5 comprehensive specs** that structured the entire evidence system architecture using Kiro's spec format: **Requirements**, **Design**, and **Implementation Tasks**.

**Spec 1: Evidence Enhancement Phase 1** (Caching, Conflict Detection, Sufficiency Scoring)
- **Requirements**: Redis caching with 24-hour TTL, conflict detection when WHO/CDC disagree, evidence sufficiency scoring (0-100 scale)
- **Design**: Cache-first architecture with graceful degradation, conflict scanner for contradictory guidelines, sufficiency scorer based on evidence hierarchy (Cochrane +30, guidelines +25, RCTs +20)
- **Impact**: Reduced latency from 5-7s to 1-2s for repeated queries, 53% cost reduction, transparent conflict reporting

**Spec 2: Evidence Enhancement Phase 2** (Semantic Search with BioBERT)
- **Requirements**: Biomedical embeddings for semantic similarity, query expansion with PICO extraction, hybrid search combining keyword + semantic results
- **Design**: PubMedBERT/BioBERT for embeddings, reranking top-50 results by cosine similarity, reciprocal rank fusion for hybrid search
- **Impact**: Found semantically similar papers that keyword matching missed, improved relevance by ~40%

**Spec 3: Evidence Enhancement Phase 3** (Chunk-Level Attribution)
- **Requirements**: Cite specific sentences (not entire papers), chunk-level semantic search, automated evaluation framework with ground truth test set
- **Design**: Split abstracts into sentences with provenance tracking (PMID + sentence index), chunk-level embeddings, citation precision/recall metrics
- **Impact**: Enabled precise citations to specific sentences, measurable quality improvements

**Spec 4: Inline Citation System** (Modern UI with Hover Cards)
- **Requirements**: Replace superscript numbers with "Sources" badges, hover cards showing reference details, dual-mode support (Doctor blue, General purple)
- **Design**: Shared component architecture with mode prop, 4-line reference structure (title link, authors, journal badge, quality badges), citation-to-reference mapping validation
- **Impact**: Improved readability while maintaining citation transparency, eliminated Google search URLs

**Spec 5: Evidence Brain Quality Fix** (PICO-First Architecture)
- **Requirements**: PICO extraction for every query, tag-based classification (disease_tags + decision_tags), anchor-aware sufficiency scoring, off-topic reference filtering
- **Design**: PICO-first pipeline where disease_tags and decision_tags drive all downstream modules (classification, ranking, sufficiency, images), query decomposition for complex questions
- **Impact**: Prevented Perplexity triggering when internal evidence sufficient, eliminated off-topic references (MINOCA in DAPT questions), fixed image selection for decision questions

**How did the spec-driven approach improve your development process?**

Specs transformed complex features from "figure it out as we go" to **systematic, phased implementation**:

**Phase 1** addressed the critical caching gap (5-7s â†’ 1-2s latency, 53% cost reduction)  
**Phase 2** tackled semantic search limitations (found papers keyword matching missed)  
**Phase 3** enabled precise sentence-level citations (not just entire papers)  
**Phase 4** modernized the UI (Sources badges, hover cards, dual-mode support)  
**Phase 5** fixed core quality issues (off-topic references, incorrect Perplexity triggering)

Each spec caught architectural issues early. For example, the **Evidence Brain Quality Fix** spec revealed that query classification was using raw text instead of extracted tags, causing lifestyle MeSH terms to be added to anticoagulation questions. By designing the PICO-first architecture in the spec, we avoided building the wrong solution.

Specs also created **living documentation**. When debugging why Perplexity was triggering despite having anchor guidelines, I could reference the Phase 5 spec's anchor-aware sufficiency scoring requirements to understand the intended behavior.

**How did this compare to vibe coding?**

**Vibe coding** worked well for:
- UI components (markdown typewriter, evidence loading cards)
- Utility functions (URL validation, citation parsing)
- Simple integrations (adding a new evidence source)

**Spec-driven development** was essential for:
- Multi-phase features (3-phase evidence enhancement)
- Complex pipelines (PICO extraction â†’ classification â†’ ranking â†’ sufficiency)
- Features with edge cases (citation validation, conflict detection)
- System-wide changes (PICO-first architecture affecting 5+ modules)

**Concrete example**: The **Inline Citation System** spec prevented a major mistake. Initially, I thought about just hiding citation markers with CSS. The spec's "Copy Behavior" requirements revealed I needed to preserve [[N]](URL) in the DOM for copyingâ€”something I would have missed with pure vibe coding and had to refactor later.

**Rule of thumb**: 
- **Vibe coding**: Features you can describe in 1-2 sentences, <3 files affected
- **Specs**: Features requiring 3+ steps, multiple modules, or significant edge cases

The 5 specs I created became the **architectural blueprint** for the entire evidence system, ensuring consistency across 50,056 lines of code.

---

### Steering Docs: Strategy & Impact

**How did you leverage steering to improve Kiro's responses?**

Steering docs were the **foundation of consistency** across 147 production files. I created three steering documents that Kiro automatically referenced in every conversation:

**tech.md** (Tech Stack Guidelines):
- Defined tech stack: Next.js 16, React 19, TypeScript 5 strict mode, Gemini 2.5 Flash, Tailwind CSS v4
- Specified patterns: Use App Router (not Pages Router), React Server Components by default, `@/*` import alias
- Encoded constraints: No Redux/Zustand (use React hooks), no custom CSS (use Tailwind), streaming responses for AI

**structure.md** (File Organization):
- Defined directory structure: `lib/evidence/` for database integrations, `components/ui/` for reusable components, `app/api/*/route.ts` for API routes
- Specified naming conventions: kebab-case for files, PascalCase for components, camelCase for functions
- Established patterns: One file per evidence source, consistent `comprehensiveSearch(query: string)` interface

**product.md** (Medical Domain Rules):
- Encoded citation requirements: Real PMIDs/DOIs only, no fabricated references
- Defined evidence hierarchy: Guidelines â†’ Systematic Reviews â†’ RCTs â†’ Observational Studies
- Specified mode differences: Doctor Mode (technical, peer-to-peer) vs General Mode (simple, educational)
- Established privacy rules: No server-side persistence, 1-hour localStorage expiration, no PHI/PII logging

**Was there a particular strategy that made the biggest difference?**

The biggest win was encoding **domain-specific rules in product.md**. Without it, I'd need to remind Kiro in every conversation: "Remember, no fabricated PMIDs," "Use the evidence hierarchy," "Doctor Mode needs technical language." With product.md, Kiro automatically followed these rules.

For example, when I asked Kiro to "add a new evidence source for Cochrane Library," it automatically:
- Created `lib/evidence/cochrane.ts` (structure.md)
- Exported `comprehensiveCochraneSearch(query: string)` (structure.md)
- Used TypeScript strict mode with proper error handling (tech.md)
- Prioritized systematic reviews in the evidence hierarchy (product.md)
- Returned structured Article objects with real DOIs (product.md)

No reminders neededâ€”steering docs handled it automatically.

Another strategy: **updating steering docs as the project evolved**. When I discovered that BioBERT semantic reranking improved relevance by ~40%, I added it to tech.md. From that point forward, Kiro automatically considered semantic reranking when generating evidence-related code.

---

### MCP: Extending Capabilities

**How did extending Kiro's capabilities help you build your project?**

I used Kiro's Model Context Protocol (MCP) integration to extend capabilities beyond code generation:

**1. fetch MCP Server** (API Testing):
- **Use case**: Testing evidence source integrations (PubMed, Cochrane, WHO, CDC, etc.) without leaving the IDE
- **Workflow**: I'd ask Kiro to "test the PubMed API with query 'diabetes type 2 treatment'" and it would use the fetch MCP server to make the actual API call, show the response, and validate the data structure
- **Impact**: Caught API integration issues early (rate limiting, malformed responses, missing fields) without writing test scripts

**2. aws-knowledge MCP Server** (Deployment):
- **Use case**: Deploying to Google Cloud Run (GCP serverless platform)
- **Workflow**: I'd ask Kiro "How do I deploy a Next.js app to Cloud Run with environment variables?" and it would reference AWS/GCP documentation through the MCP server
- **Impact**: Got accurate, up-to-date deployment instructions without leaving the IDE or searching documentation

**What sort of features or workflow improvements did MCP enable that otherwise would have been difficult or impossible?**

**Real-time API testing** was the biggest win. Without MCP, I'd need to:
1. Write a test script
2. Run it in terminal
3. Parse the output
4. Debug issues
5. Repeat

With the fetch MCP server, I could test APIs conversationally: "Test the Cochrane API with query 'sepsis treatment' and show me the response structure." Kiro would make the call, parse the response, and suggest improvements to my integration codeâ€”all in one conversation.

This was especially valuable for the 57+ database integrations. Each database has different API patterns, rate limits, and response formats. The fetch MCP server let me test each one interactively and iterate quickly.

Another workflow improvement: **documentation lookup without context switching**. Instead of opening browser tabs to search Next.js docs, TypeScript docs, or Gemini API docs, I'd ask Kiro through MCP. This kept me in flow state and reduced context switching.

**Example**: When implementing streaming responses, I asked "How do I create a ReadableStream in Next.js 16 App Router?" Kiro used MCP to reference Next.js documentation and generated the correct pattern with `TextEncoder` and `controller.enqueue()`.

Without MCP, these workflows would require manual testing, documentation searching, and context switchingâ€”slowing development significantly.

---

**Total Word Count (including original + Q&A)**: ~2,400 words


---

## LinkedIn Post - Social Blitz Prize Submission

### The Post:

Have you ever watched friends or family frantically Google symptoms at 2 AM, spiraling into anxiety from conflicting medical advice? Or seen doctors waste hours sifting through research papers while patients wait?

I built an app to solve this: **MedGuidance AI** â€” an evidence-based medical assistant that integrates 57+ authoritative medical databases (PubMed, Cochrane, WHO, CDC, NICE, FDA) to provide verified medical information for both healthcare professionals and the general public.

**Unlike ChatGPT or other AI assistants that hallucinate references and cite outdated guidelines, MedGuidance AI achieves <1% hallucination rate** by validating every citation against real PMIDs, DOIs, and guideline URLs.

**From idea to production in weeks** â€” all thanks to @kirodotdev ðŸš€

Here's how Kiro transformed my development process:

**The Challenge:** Building a medical AI demands precision. I needed to integrate 57+ databases, implement multi-stage RAG pipelines, validate citations server-side, and maintain architectural consistency across 50,056 lines of code. Doing this manually would take months.

**The Kiro Difference:**

Kiro didn't just write code faster â€” it understood the entire architecture and guided me from concept to production:

ðŸŽ¯ **Steering Docs** became my architectural blueprint. I defined tech stack (Next.js 16, React 19, Gemini 2.5 Flash), file organization patterns, and medical domain rules (citation requirements, evidence hierarchy, privacy). Kiro automatically followed these rules across 147 files â€” no architectural drift, no "which pattern should I use?" decisions.

ðŸ“‹ **Spec-Driven Development** caught mistakes before I wrote code. When designing the Evidence Brain Quality Fix, the spec revealed I was using raw query text instead of extracted PICO tags for classification â€” would have been a costly refactor. Specs became living documentation explaining why features exist and what edge cases they handle.

ðŸ¤– **8 Agent Hooks** automated quality control I didn't know I needed. The Hallucination Detector caught off-topic references (MINOCA in DAPT questions) that I would have missed. The Clinical Rigor Enforcer maintained OpenEvidence-level standards automatically. Every file save triggered validation â€” tight feedback loop, zero cognitive load.

**The Results:**
â€¢ <1% hallucinated citations (down from 30%)
â€¢ 87% faster evidence gathering (45s â†’ 5-7s)
â€¢ 93%+ medical image analysis accuracy
â€¢ 50,056 lines of production code in weeks, not months

Kiro isn't just a coding assistant. It's a development partner that understands context, maintains quality, and scales with complexity. It elevated my entire development process â€” from reactive debugging to proactive architecture.

Medical AI demands precision. Lives depend on it. With Kiro, I built something I'm proud to put in front of doctors and patients.

ðŸ”— Try it live: https://medguidance-ai-473674535154.us-central1.run.app/

#hookedonkiro #medicalai #healthtech #aiinhealthcare #evidencebasedmedicine #kiro

---

**Character Count:** ~2,450 characters (well within LinkedIn's 3,000 limit)

**Tags Used:**
- @kirodotdev (required, placed naturally in context)
- #hookedonkiro (required)
- Additional relevant hashtags for reach

**Post Strategy:**
- **Hook**: Relatable opening (friends/family Googling symptoms at 2 AM)
- **Problem-Solution**: Medical misinformation â†’ MedGuidance with 57+ databases
- **Differentiation**: "Unlike ChatGPT" comparison with <1% hallucination rate
- **Kiro Journey**: "From idea to production in weeks" with detailed explanation
- **The Challenge**: Sets up why this was hard (medical precision, 57+ databases, 50K LOC)
- **The Kiro Difference**: 3 features with concrete examples of how Kiro understood and guided
- **Quantified Results**: 4 measurable outcomes
- **Emotional Close**: "Lives depend on it" connects to real-world stakes
- **Call-to-Action**: Live demo link

**Why This Will Stand Out:**
1. **Relatable Opening**: Everyone has Googled symptoms â€” instant connection
2. **Clear Differentiation**: "Unlike ChatGPT" comparison shows competitive advantage
3. **Journey Narrative**: "From idea to production" shows Kiro's role throughout
4. **Architectural Intelligence**: Goes beyond "faster coding" to show Kiro understood the concept
5. **Concrete Examples**: Steering Docs, Specs, Agent Hooks with real impact
6. **Quantified Results**: Numbers tell the story (30% â†’ <1%, 87% faster, 50K LOC)
7. **Real-World Stakes**: Medical AI context shows serious application
8. **Visual Support**: 3 images showing landing page, General Mode, Doctor Mode

**Posting Tips:**
- Post during peak LinkedIn hours (Tuesday-Thursday, 8-10 AM or 12-1 PM)
- Engage with comments quickly to boost algorithm visibility
- Share in relevant LinkedIn groups (AI, HealthTech, Developer communities)
- Consider tagging relevant connections who work in medical/AI fields
- Pin this post to your profile for maximum visibility during judging period


---

## DEV.to Blog Post - Building MedGuidance AI with Kiro

### Title: I Built a Medical AI Assistant in Weeks That Would Have Taken Months â€” Here's How Kiro Made It Possible

---

### The Pain Point That Started It All

I'll never forget Googling "lower back pain + leg numbness" at midnight and spiraling through 47 contradictory Reddit threads, sketchy blogs, and fear-mongering articles.

**Zero evidence. Zero citations. Just panic.**

On the flip side: doctors spend 5-6 hours reading research papers to help one patient. The evidence existsâ€”it's just buried in 40+ databases across PubMed, Cochrane, WHO, CDC, and dozens of other sources.

So I built **MedGuidance AI**: an evidence-based medical assistant that does the heavy lifting for both sides.

---

### What is MedGuidance AI?

MedGuidance AI is a production-grade medical AI assistant that integrates **57+ authoritative medical databases** with Google Gemini AI to provide verified, evidence-based medical information. Unlike ChatGPT or other AI assistants that hallucinate references and cite outdated guidelines, MedGuidance AI achieves **<1% hallucination rate** by validating every citation against real PMIDs, DOIs, and guideline URLs.

The system serves two distinct audiences through specialized modes:

---

### ðŸ¥ Doctor Mode: Clinical Decision Support for Healthcare Professionals

![Doctor Mode - Medical Image Analysis](https://medguidance-ai-473674535154.us-central1.run.app/doctor-mode-screenshot.png)

**Doctor Mode** is a clinical research copilot designed for healthcare professionals, medical students, and researchers who need fast access to evidence-based medical information.

#### Key Features:

**1. Verified Citations with Real PMIDs/DOIs**
Every medical claim is backed by real references from 57+ databases:
- PubMed (40M+ articles)
- Cochrane Library (systematic reviews)
- WHO, CDC, NICE Guidelines
- ClinicalTrials.gov
- Europe PMC, OpenFDA, DailyMed
- And 50+ more authoritative sources

Example query: *"Anticoagulation for AF patient with CKD stage 4"*

Response surfaces:
- NICE Guideline CG180 (2021): Atrial Fibrillation Management
- ACC/AHA/ACCP/HRS 2023 AF Guidelines
- PMID: 28122885 - Apixaban dosing in severe renal impairment
- PMID: 31378705 - DOACs vs warfarin in advanced CKD
- Recommendation: Apixaban 2.5mg BID with dose adjustment criteria

**2. Anchor Guidelines System**
Pre-defined gold-standard guidelines for 12+ common clinical scenarios:
- Sepsis (Surviving Sepsis Campaign 2021)
- Community-Acquired Pneumonia (IDSA/ATS 2019)
- Diabetes + CKD (ADA 2024, KDIGO 2022)
- Heart Failure (ACC/AHA/HFSA 2022)
- Atrial Fibrillation (ACC/AHA/ACCP/HRS 2023)

**3. Medical Image Analysis (93%+ Accuracy)**
Upload X-rays, CT scans, or MRIs for AI-powered analysis with:
- **Thermal heatmaps** highlighting regions of interest
- **Bounding box annotations** for specific findings
- **Multi-stage vision pipeline**: MedGemma â†’ Advanced Vision â†’ Standard Gemini fallback
- **Anatomical landmark detection** with 95%+ precision

The image shows a brain MRI analysis with thermal heatmap overlay, demonstrating how the AI identifies and highlights the frontal convexity extra-axial mass with precise localization.

**4. 3-Tab Structured Responses**
- **Clinical Analysis**: Comprehensive clinical overview
- **Diagnosis & Logic**: Differential diagnosis and clinical reasoning
- **Treatment & Safety**: Evidence-based treatment recommendations

**5. Clinical Decision Support**
Auto-triggered safety checks for:
- Psychiatric emergencies and suicide risk assessment
- QT-prolonging medications
- Adolescent care coordination

---

### ðŸ‘¥ General Mode: Consumer-Friendly Health Information

![General Mode - Health Information](https://medguidance-ai-473674535154.us-central1.run.app/general-mode-screenshot.png)

**General Mode** translates complex medical evidence into simple, actionable advice for the general public.

#### Key Features:

**1. Plain-English Explanations**
No medical jargonâ€”just clear, understandable health information.

Example query: *"What are the common causes of lower back pain with leg radiation (sciatica)?"*

Response includes:
- **What's Going On**: Possible causes explained simply (herniated disc, spinal stenosis, muscle strain)
- **Best Things You Can Do at Home**: Rest, heat/ice therapy, gentle movement, over-the-counter pain relievers
- **Easy Ways to Move More**: Gentle walking, stretching exercises with descriptions
- **When to See a Doctor**: Persistent pain >3 days, fever, loss of bladder control

The image shows the response with anatomical diagrams and medical imaging (MRI scans) to help users understand their condition visually.

**2. Crisis Detection (<100ms Response)**
Two-level safety system:
- **Level 1 (Explicit Self-Harm)**: Immediate crisis intervention with hotline numbers (988, Samaritans)
- **Level 2 (Hidden Distress)**: Soft safety guidance with connection advice

**3. Actionable Advice**
- Helpful exercises with step-by-step instructions
- Dietary suggestions based on WHO/CDC guidelines
- Lifestyle modifications backed by evidence

**4. Safety-First Design**
Every response includes:
- "When to See a Doctor" guidance
- Emergency warning signs
- Safety disclaimers
- Educational focus (not diagnosis)

---

### ðŸ”¬ How It Works: The Evidence Engine

The heart of MedGuidance AI is its sophisticated evidence gathering pipeline:

1. **PICO Extraction**: Identifies disease tags and decision tags from queries
2. **Scenario Detection**: Classifies queries into clinical scenarios (HFpEF, AF+CKD, etc.)
3. **Anchor Guidelines Injection**: Automatically injects gold-standard guidelines
4. **Parallel Evidence Search**: Queries 57+ databases simultaneously using `Promise.all()` (5-7 seconds)
5. **Semantic Reranking**: BioBERT-based relevance scoring
6. **Sufficiency Scoring**: Determines if evidence is sufficient
7. **AI Synthesis**: Gemini 2.5 Flash synthesizes evidence into structured responses
8. **Citation Validation**: Server-side validation ensures every citation exists

**Evidence Hierarchy**:
1. Clinical practice guidelines (WHO, CDC, NICE, ACC/AHA)
2. Systematic reviews and meta-analyses (Cochrane)
3. Randomized controlled trials (ClinicalTrials.gov, PubMed)
4. Observational studies and case reports

---

### ðŸš€ The Secret Weapon: Kiro AI

Here's the truth: **I built 50,056 lines of production-ready TypeScript in weeks, not months**â€”all thanks to Kiro AI.

Kiro didn't just help me code faster. It understood the entire architecture and guided me from concept to production.

#### How Kiro Transformed Development:

**1. Steering Docs: Architectural Consistency**

Before writing a single line of code, I created three steering documents:

- **tech.md**: Tech stack (Next.js 16, React 19, TypeScript 5, Gemini 2.5 Flash)
- **structure.md**: File organization (evidence sources, API routes, components)
- **product.md**: Medical domain rules (citation requirements, evidence hierarchy, privacy)

Kiro automatically followed these rules across **147 files**â€”no architectural drift, no "which pattern should I use?" decisions.

**2. Spec-Driven Development: Caught Mistakes Before Coding**

For complex features, I used Kiro's spec system to formalize requirements before implementation:

- **Evidence Enhancement Phase 1**: Caching, conflict detection, sufficiency scoring
- **Evidence Enhancement Phase 2**: Semantic search with BioBERT
- **Evidence Enhancement Phase 3**: Chunk-level attribution
- **Inline Citation System**: Modern UI with hover cards
- **Evidence Brain Quality Fix**: PICO-first architecture

The **Evidence Brain Quality Fix** spec revealed I was using raw query text instead of extracted PICO tags for classificationâ€”would have been a costly refactor if I'd coded first.

**3. Agent Hooks: Self-Maintaining Precision**

This is my **favorite Kiro feature**: Agent Hooks that turn development from manual synchronization chaos into self-maintaining precision.

I created **8 specialized agent hooks** that automated quality control:

- **Code Quality Analyzer**: Caught long functions, missing types, inefficient algorithms
- **Hallucination Detector**: Prevented off-topic references (MINOCA in DAPT questions)
- **Medical Image Quality Enforcer**: Validated bounding box accuracy
- **Doctor Mode Clinical Rigor Enforcer**: Maintained OpenEvidence-level standards
- **General Mode Safety Net Validator**: Ensured crisis detection worked
- **Evidence Routing Validator**: Prevented off-balance evidence stacks
- **Documentation Sync**: Kept docs current automatically
- **Clinical Scenario Microprompt Library Manager**: Maintained scenario-specific guidance

**Real Example**: When I updated the evidence pipeline to add landmark trials, Kiro automatically:
- Regenerated **28 documentation files**
- Validated **17 new citations**
- Updated TypeScript interfaces across **12 modules**

**All in 47 seconds.**

---

### ðŸ“Š The Results

**Development Metrics**:
- **50,056 lines** of production-ready TypeScript
- **147 production files** with architectural consistency
- **57+ medical database integrations** with parallel search
- **~75% faster** development time compared to traditional coding
- **Production-ready from day one**

**Quality Metrics**:
- **<1% hallucinated citations** (down from 30%)
- **87% faster evidence gathering** (45s â†’ 5-7s)
- **93%+ medical image analysis accuracy**
- **<100ms crisis response** for self-harm detection

**Architecture Achievements**:
- Multi-stage RAG pipeline with PICO extraction
- Citation validation system with server-side enforcement
- Dual-mode architecture serving doctors and patients
- Privacy-first design (HIPAA-friendly, GDPR-compliant)

---

### ðŸ›¡ï¸ Privacy & Safety First

**Privacy**:
- No server-side data storage
- 1-hour automatic deletion from localStorage
- Zero PII logging
- Client-side image processing only

**Safety**:
- Crisis detection for self-harm phrases
- QT-risk medication alerts
- Psychiatric risk assessment
- Adolescent care templates
- "When to See a Doctor" guidance in every General Mode response

---

### ðŸ’¡ What I Learned

**About Kiro**:
- **Spec-driven development** beats pure "vibe coding" for complex features
- **Steering docs** are essential for maintaining consistency across large codebases
- **Agent hooks** eliminate cognitive load and automate quality control
- Kiro isn't just a coding assistantâ€”it's a **development partner** that understands context

**About Medical AI**:
- **Evidence hierarchy matters**: Guidelines â†’ SRs â†’ RCTs â†’ Observational Studies
- **Server-side validation is non-negotiable**: A single fabricated reference undermines trust
- **Privacy must be built-in, not bolted-on**: No server-side persistence, no exceptions

**About RAG Systems**:
- **PICO query decomposition** dramatically improves retrieval quality
- **BioBERT semantic reranking** improved relevance by ~40%
- **Sufficiency scoring** prevents over-fetchingâ€”not every query needs 57 databases
- **Anchor guidelines** ensure consistent, high-quality responses

---

### ðŸŽ¯ What's Next

**Short-Term (3-6 Months)**:
- Trial with medical students and healthcare professionals
- Performance optimization (8-12s â†’ 3-5s response time)
- Mobile-first experience

**Medium-Term (6-12 Months)**:
- Specialty-specific modes (Cardiology, Neurology, Pediatrics, Emergency Medicine)
- Clinical workflow integration (browser extensions, mobile apps)
- Collaborative features for medical teams

**Long-Term (12+ Months)**:
- Hospital partnerships
- EHR integration (Epic, Cerner)
- CME-accredited content
- FDA clearance as a Clinical Decision Support System

**The Vision**: MedGuidance AI becomes the trusted companion for every medical professionalâ€”from medical students learning clinical reasoning to attending physicians making critical treatment decisions.

---

### ðŸ”— Try It Yourself

**MedGuidance AI** (desktop only): [https://medguidance-ai-473674535154.us-central1.run.app/](https://medguidance-ai-473674535154.us-central1.run.app/)

**Kiro AI**: [https://kiro.dev/](https://kiro.dev/)

**GitHub**: [Repository Link]

---

### âš ï¸ Important Disclaimer

MedGuidance AI is an **early beta** educational tool and decision support system. It is **NOT a substitute for professional medical advice, diagnosis, or treatment**. Always consult qualified healthcare professionals when making medical decisions.

---

### ðŸ™ Thank You, Kiro

This is what AI-assisted development looks like when done right. Kiro didn't just speed up codingâ€”it elevated the entire development process from reactive debugging to proactive architecture.

**@kirodotdev** â€” you made this possible. Thank you! ðŸ™

---

### ðŸ—ï¸ Built With

- **Next.js 16** + **React 19** (App Router, Server Components)
- **TypeScript 5** (strict mode)
- **Google Gemini 2.5 Flash** (text generation)
- **Google Gemini 2.0 Flash Exp** (medical vision analysis)
- **Tailwind CSS v4** + **shadcn/ui**
- **57+ Medical APIs** (PubMed, Cochrane, WHO, CDC, NICE, FDA, and more)

**License**: MIT (Open Source)

---

### ðŸ“¢ Join the Conversation

What's your experience with AI-assisted development? Have you tried Kiro? Drop a comment below!

**Hashtags**: #hookedonkiro #kiro #medicalai #healthtech #evidencebasedmedicine #digitalhealth #aiinhealthcare #clinicaldecisionsupport #medtech #nextjs #react #typescript #googlegemini

---

**Word Count**: ~1,800 words (optimal for DEV.to engagement)

**Images to Include**:
1. Landing page screenshot (mode selection)
2. General Mode screenshot (lower back pain query with anatomy diagrams and MRI scans)
3. Doctor Mode screenshot (brain MRI analysis with thermal heatmap and bounding boxes)
4. Evidence gathering animation (optional)
5. Citation system screenshot (optional)

**SEO Keywords**: medical AI, evidence-based medicine, Kiro AI, clinical decision support, healthcare technology, AI-assisted development, medical image analysis, citation validation, RAG systems, Next.js medical app
